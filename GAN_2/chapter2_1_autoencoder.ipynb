{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오토 인코더의 정의와 구조 (Definition and Structures of AutoEncoder)\n",
    "\n",
    "- 오토 인코더의 정의\n",
    "    - 차원 축소 등을 위해 표현학습(Representation Learning)또는 특징학습(Feature learning)을 `비지도 학습`의 형태로 학습하는 신경망\n",
    "\n",
    "- Encoder : 입력값을 받아 특징값으로 변환하는 신경망\n",
    "    - action , feature을 뽑아내는 과정\n",
    "- Decoder : 특징값을 출력값으로 반환하는 신경망\n",
    "    - latent feature가 진짜 잠재된 feature인지 학습하기 위해서\n",
    "- Latent feature : 신경망 내부에서 추출된 특징적 값들.\n",
    "\n",
    "> 표현학습(Represenation Learning)/특징학습(Feature learning)\n",
    "- `특징탐지(feature detection)`이나 `분류(classification)`를 위해 필요한 `특징점`을 `자동`으로 발견하는 시스템적 기법들의 총체\n",
    "    - Taxonomy of represenation learning\n",
    "        - 비지도학습 : K-means Clustering // Principal Component Analysis // Local Linear Embedding // Independecncs Component Analysis // Unsupervised Dictionary Learning ...\n",
    "        - 지도학습 : Supervised Dictionary Learning // Neural Networks\n",
    "        - 다층 구조 학습 : Resitricted Boltzman Machine // Autoencoder\n",
    "\n",
    "> 차원축소를 하는 이유 : 차원의 저주 (The Reason forb dimension-reducing: curse of diemnsionality)\n",
    "- 고차원 공간의 데이터를 분석하거나 측정을 할 때 저차원 공간에서 나타나지 않았던 여러 문제들이 발생하는 것으로, 머신러닝에서 일반적으로 차원이 증가할 경우 기하급수적인 데이터가 요구되는 현상을 말한다.\n",
    "\n",
    "> 차원축소 해도 문제가 없나 ? Manifold Hypothesis\n",
    "\n",
    "1. Manifold Hypothesis의 정의\n",
    "- 고차원의 데이터가 저차원의 다양체(Manifold)에 표현될 수 있다는 것을 가정하는 것을 말한다.\n",
    "- 다양체 : 기하학적인 유추를 통하여 4차원 이상의 공간을 연구하기위해 도입된 개념. 점 직선,평면,원,삼각형,입체,구 와같은 기하학적 도형의 집합을 1개의 공간으로 보았을 때의 공간을 말한다.\n",
    "\n",
    "- dense layer는 개발자가 원하는 demension을 가지고온다.\n",
    "- 차원을 내려가는 layer들을 encoder라한다.\n",
    "- Latent vector : 가장 낮은 차원의 데이터(image의 feature을 나타냄)\n",
    "- 차원을 늘려가는 layer들을 decoder라한다. : latent vector로 원본으로 그려준다.\n",
    "\n",
    "- ref : https://www.youtube.com/watch?v=JTD_q4wuw_4&t=151s\n",
    "    - https://github.com/minsuk-heo/tf2/blob/master/jupyter_notebooks/04.AutoEncoder.ipynb\n",
    "\n",
    "https://www.youtube.com/watch?v=SAfJz_uzaa8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(60000,784)\n",
    "\n",
    "x_test = x_test[:300]\n",
    "y_tset = y_test[:300]\n",
    "x_test = x_test.reshape(300,784)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#nomalization\n",
    "\n",
    "gray_scale = 255\n",
    "x_train/=gray_scale\n",
    "x_train/=gray_scale\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoder1 = Dense(128,activation = 'sigmoid')(input_img)\n",
    "encoder2 = Dense(784,activation = 'sigmoid')(encoder1)\n",
    "\n",
    "decoder = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders의 모든것\n",
    "\n",
    "> AutoEncoders - A way for unspupervised Learning of Nonlinear Manifold\n",
    "\n",
    "## Keyword\n",
    "- unsupervised learning\n",
    "- represenation learning\n",
    "- effficient coding learning\n",
    "- `dimensionality reduction`\n",
    "- generative model learning\n",
    "\n",
    "> `dimensionality reduction`\n",
    "## Keyword\n",
    "- Unsupervised learning\n",
    "- Nonlinear Dimensionality reduction\n",
    "    - represenation learning\n",
    "    - efficient coding learning\n",
    "    - feature extraction\n",
    "    - manifold learning\n",
    "- generative model learning\n",
    "- `Maimaum Likelihood density estimation`\n",
    "\n",
    "[main keywords]\n",
    "- unsupervised learning\n",
    "- manifold learning\n",
    "- generative model learning\n",
    "- ML density estimaation\n",
    "\n",
    "- 오토인코더를 학습할 때:\n",
    "- 학습방법은 비교사 학습방법을 따르며,(unsupervised learning), loss는 negative ML로 해석된다(ML density estimation)\n",
    "\n",
    "- 학습된 오토인코더에서:\n",
    "- 인코더는 차원축소 역활을 수행하며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Revisit Depp Neural Networks\n",
    "    - Machine learning problem\n",
    "    -\n",
    "2. Manifold Learning\n",
    "3. Autoencoders\n",
    "4. Variational Autoencoders\n",
    "5. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chapter1 Revisit Deep Neural Networks\n",
    "- Machine learning problem\n",
    "- Loss function viewpoints 1 : Back-propagation\n",
    "- Loss function viewpoints 2 : Maximum likelihood\n",
    "- Maximum likelihood for autoencoders\n",
    "\n",
    "- keyword : ML density estimation\n",
    "    - 딥 뉴렬넷을 학스할 떄 사용되는 loss function의 다양한 각도에서 해석할 수 있다.\n",
    "    - 그중 하나는 back-propagation 알고리즘이 좀 더 잘 작동할 수 있는지 해석한다.(gradient-vanishing problem)이 덜 발생할 수 있따는 해석\n",
    "    - negative maximum likelihood로 보고 특정 형태의 loss는 특정 형태의 확률분포를 가정한다는 해석\n",
    "    - Autoencoder를 학습하는 것 또는 maximum likelihood관점에서의 최적화로 볼 수 있다.\n",
    "\n",
    "ㅔ"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5ebc6cf6f7fb120347a232b371902f436be28b211613027002426b5b4c265db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensor-mac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
