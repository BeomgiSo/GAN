{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오토 인코더의 정의와 구조 (Definition and Structures of AutoEncoder)\n",
    "\n",
    "- 오토 인코더의 정의\n",
    "    - 차원 축소 등을 위해 표현학습(Representation Learning)또는 특징학습(Feature learning)을 `비지도 학습`의 형태로 학습하는 신경망\n",
    "\n",
    "- Encoder : 입력값을 받아 특징값으로 변환하는 신경망\n",
    "    - action , feature을 뽑아내는 과정\n",
    "- Decoder : 특징값을 출력값으로 반환하는 신경망\n",
    "    - latent feature가 진짜 잠재된 feature인지 학습하기 위해서\n",
    "- Latent feature : 신경망 내부에서 추출된 특징적 값들.\n",
    "\n",
    "> 표현학습(Represenation Learning)/특징학습(Feature learning)\n",
    "- `특징탐지(feature detection)`이나 `분류(classification)`를 위해 필요한 `특징점`을 `자동`으로 발견하는 시스템적 기법들의 총체\n",
    "    - Taxonomy of represenation learning\n",
    "        - 비지도학습 : K-means Clustering // Principal Component Analysis // Local Linear Embedding // Independecncs Component Analysis // Unsupervised Dictionary Learning ...\n",
    "        - 지도학습 : Supervised Dictionary Learning // Neural Networks\n",
    "        - 다층 구조 학습 : Resitricted Boltzman Machine // Autoencoder\n",
    "\n",
    "> 차원축소를 하는 이유 : 차원의 저주 (The Reason forb dimension-reducing: curse of diemnsionality)\n",
    "- 고차원 공간의 데이터를 분석하거나 측정을 할 때 저차원 공간에서 나타나지 않았던 여러 문제들이 발생하는 것으로, 머신러닝에서 일반적으로 차원이 증가할 경우 기하급수적인 데이터가 요구되는 현상을 말한다.\n",
    "\n",
    "> 차원축소 해도 문제가 없나 ? Manifold Hypothesis\n",
    "\n",
    "1. Manifold Hypothesis의 정의\n",
    "- 고차원의 데이터가 저차원의 다양체(Manifold)에 표현될 수 있다는 것을 가정하는 것을 말한다.\n",
    "- 다양체 : 기하학적인 유추를 통하여 4차원 이상의 공간을 연구하기위해 도입된 개념. 점 직선,평면,원,삼각형,입체,구 와같은 기하학적 도형의 집합을 1개의 공간으로 보았을 때의 공간을 말한다.\n",
    "\n",
    "- dense layer는 개발자가 원하는 demension을 가지고온다.\n",
    "- 차원을 내려가는 layer들을 encoder라한다.\n",
    "- Latent vector : 가장 낮은 차원의 데이터(image의 feature을 나타냄)\n",
    "- 차원을 늘려가는 layer들을 decoder라한다. : latent vector로 원본으로 그려준다.\n",
    "\n",
    "- ref : https://www.youtube.com/watch?v=JTD_q4wuw_4&t=151s\n",
    "    - https://github.com/minsuk-heo/tf2/blob/master/jupyter_notebooks/04.AutoEncoder.ipynb\n",
    "\n",
    "https://www.youtube.com/watch?v=SAfJz_uzaa8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(60000,784)\n",
    "\n",
    "x_test = x_test[:300]\n",
    "y_tset = y_test[:300]\n",
    "x_test = x_test.reshape(300,784)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#nomalization\n",
    "\n",
    "gray_scale = 255\n",
    "x_train/=gray_scale\n",
    "x_train/=gray_scale\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoder1 = Dense(128,activation = 'sigmoid')(input_img)\n",
    "encoder2 = Dense(784,activation = 'sigmoid')(encoder1)\n",
    "\n",
    "decoder = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders의 모든것\n",
    "\n",
    "> AutoEncoders - A way for unspupervised Learning of Nonlinear Manifold\n",
    "\n",
    "## Keyword\n",
    "- unsupervised learning\n",
    "- represenation learning\n",
    "- effficient coding learning\n",
    "- `dimensionality reduction`\n",
    "- generative model learning\n",
    "\n",
    "> `dimensionality reduction`\n",
    "## Keyword\n",
    "- Unsupervised learning\n",
    "- Nonlinear Dimensionality reduction\n",
    "    - represenation learning\n",
    "    - efficient coding learning\n",
    "    - feature extraction\n",
    "    - manifold learning\n",
    "- generative model learning\n",
    "- `Maimaum Likelihood density estimation`\n",
    "\n",
    "[main keywords]\n",
    "- unsupervised learning\n",
    "- manifold learning\n",
    "- generative model learning\n",
    "- ML density estimaation\n",
    "\n",
    "- 오토인코더를 학습할 때:\n",
    "- 학습방법은 비교사 학습방법을 따르며,(unsupervised learning), loss는 negative ML로 해석된다(ML density estimation)\n",
    "\n",
    "- 학습된 오토인코더에서:\n",
    "- 인코더는 차원축소 역활을 수행하며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Revisit Depp Neural Networks\n",
    "    - Machine learning problem\n",
    "    -\n",
    "2. Manifold Learning\n",
    "3. Autoencoders\n",
    "4. Variational Autoencoders\n",
    "5. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chapter1 Revisit Deep Neural Networks\n",
    "- Machine learning problem\n",
    "- Loss function viewpoints 1 : Back-propagation\n",
    "- Loss function viewpoints 2 : Maximum likelihood\n",
    "- Maximum likelihood for autoencoders\n",
    "\n",
    "- keyword : ML density estimation\n",
    "    - 딥 뉴렬넷을 학스할 떄 사용되는 loss function의 다양한 각도에서 해석할 수 있다.\n",
    "    - 그중 하나는 back-propagation 알고리즘이 좀 더 잘 작동할 수 있는지 해석한다.(gradient-vanishing problem)이 덜 발생할 수 있따는 해석\n",
    "    - negative maximum likelihood로 보고 특정 형태의 loss는 특정 형태의 확률분포를 가정한다는 해석\n",
    "    - Autoencoder를 학습하는 것 또는 maximum likelihood관점에서의 최적화로 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML Problem | Classic Machine Learning\n",
    "1. Collection training data\n",
    "    - $x = \\{x_{1},x_{2},x_{3} \\cdots x_{N}\\}$\n",
    "    - $y = \\{y_{1},y_{2},y_{3} \\cdots y_{N}\\}$\n",
    "    - $D = \\{(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}) \\cdots (x_{N},y_{N})\\}$\n",
    "\n",
    "2. Define functions\n",
    "    - output : $f_{\\theta}(x)$\n",
    "    - Loss : $L(f_{\\theta}(x),y)$\n",
    "\n",
    "3. Learning/training\n",
    "    - find the optimal parameter\n",
    "\n",
    "4. predicting/testing\n",
    "    - compute optimal function output\n",
    "\n",
    "- input -> model -> output\n",
    "- model -> 모델의 종류 $f_{\\theta}(\\cdot)$\n",
    "- loss measure : $L(f_{\\theta}(x),y)$\n",
    "\n",
    "- $\\theta^{*} = argmin_{\\theta}L(f_{\\theta}(x),y)$ backpropagation to find optimal parameter\n",
    "- $y_{new} = f_{\\theta}(x_{new})$ to find optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML Problem | Deep Neural Networks\n",
    "- definition functions(models)\n",
    "- backpropagation을 통해 DNN학습을 시키기 위한 조건들\n",
    "- Loss function을 쓰기위해서\n",
    "- Assumption 1 : total loss of DNN over training samples is `the sum` of loss for each training sample\n",
    "- Assumption 2 : Loss for each training example a function of final output of DNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML Problem | Deep Neural Networks\n",
    "- $\\theta^{*} = argmin_{\\theta}L(f_{\\theta}(x),y)$  -> Gradient descent\n",
    "- Iterative method\n",
    "- $\\theta^{*} = argmin_{\\theta}L(f_{\\theta}(x),y)=$\n",
    "    1. 어떻게 $\\theta$를 바꿀껀지? \n",
    "    2. 언제까지 변화해 줄 것인지?\n",
    "    - 로스값이 줄어드는 방향으로 계속 이동하고, 움직여도 로스값이 변함이 없을 경우 멈춘다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML Problem | Deep Neural Networks | learning/Training\n",
    "- Talyer Expansion // Approximation\n",
    "- $\\nabla L$ is gradient of $L$ and indicates the steepest increasing direction of $L$\n",
    "- 즉 - gradient가 되어야 loss가 줄어지는 방향으로 파라미터를 바꿔줌(learning rate)\n",
    "- learning rate를 사용하여 조금씩 파라미터 값을 바꾸는 것을 로스 함수의 1차 미분할꺼지만 사용했기에 아주 좁은 영역에서만 감소하는 방향이 정확하기 떄문에\n",
    "\n",
    "- 전체 데이터에 대한 로스 함수의 각 데이터 샘프에 대한 로스의 합으로 구성되어 있기 떄문에 미분 계산을 효울적으로 할 수 있다.\n",
    "- 만약 곱으로 구성되어 으면 미분을 위한 모든 샘플의 결과를 메모리에 저장해야 한다.\n",
    "- 원래는 모든 데이터에 대한 로스 미분값의 합을 구한 후 파라미터를 갱신해야 하지만, 배치 크기만큼만 로스 미분값의 합을 구한 파라미터를 갱신한다.\n",
    "\n",
    "- 전체 DB가 너무 크기 때문에 소규모 DB를 통해 gradient를 계산해준다(Stochastic Gradient Descent) mini batch크기로\n",
    "\n",
    "- backpropagation\n",
    "- 특정 레이어에서 파라미터 갱신식\n",
    "- 로스함수의 미분값이 딥 뉴럴넷 학습에서 제일 중요!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LOSS FUNCTION | VIEW-POINT 1\n",
    "- Type 1 : Mean Square Error / Quadratic loss\n",
    "- classification : cross entropy\n",
    "- continuate information : mean sqaure\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5ebc6cf6f7fb120347a232b371902f436be28b211613027002426b5b4c265db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensor-mac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
